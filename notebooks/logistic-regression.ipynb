{"cells":[{"cell_type":"code","execution_count":null,"id":"a59ce6ae","metadata":{"id":"a59ce6ae","outputId":"3f5bfa6d-17fe-473d-f6cd-05fd3ad45e48"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>file_path</th>\n","      <th>file_size</th>\n","      <th>line_count</th>\n","      <th>extension</th>\n","      <th>language</th>\n","      <th>code</th>\n","      <th>clean_code</th>\n","      <th>clean_line_count</th>\n","      <th>clean_size</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>Markdown/000001.md</td>\n","      <td>34784</td>\n","      <td>572</td>\n","      <td>md</td>\n","      <td>Markdown</td>\n","      <td># Contributing\\n\\n| Component            | Bui...</td>\n","      <td>contributing\\n\\n component             build ...</td>\n","      <td>186</td>\n","      <td>10000</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2</td>\n","      <td>XML/000002.props</td>\n","      <td>3013</td>\n","      <td>44</td>\n","      <td>props</td>\n","      <td>XML</td>\n","      <td>﻿&lt;Project ToolsVersion=\"15.0\" xmlns=\"http://sc...</td>\n","      <td>project toolsversion xmlns\\n  propertygroup\\n ...</td>\n","      <td>44</td>\n","      <td>1812</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>3</td>\n","      <td>Text/000003.txt</td>\n","      <td>1076</td>\n","      <td>21</td>\n","      <td>txt</td>\n","      <td>Text</td>\n","      <td>The MIT License (MIT)\\n\\nCopyright (c) 2015 Mi...</td>\n","      <td>the mit license mit\\n\\ncopyright c 2015 micros...</td>\n","      <td>21</td>\n","      <td>1026</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>4</td>\n","      <td>Markdown/000004.md</td>\n","      <td>8105</td>\n","      <td>84</td>\n","      <td>md</td>\n","      <td>Markdown</td>\n","      <td># Azure SDK for .NET\\n\\n[![Packages](https://i...</td>\n","      <td>azure sdk for net\\n\\npackageshttpsimgshieldsi...</td>\n","      <td>84</td>\n","      <td>7244</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>5</td>\n","      <td>Markdown/000005.md</td>\n","      <td>2763</td>\n","      <td>41</td>\n","      <td>md</td>\n","      <td>Markdown</td>\n","      <td>&lt;!-- BEGIN MICROSOFT SECURITY.MD V0.0.5 BLOCK ...</td>\n","      <td>begin microsoft securitymd v005 block \\n\\n se...</td>\n","      <td>41</td>\n","      <td>2523</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   id           file_path  file_size  line_count extension  language  \\\n","0   1  Markdown/000001.md      34784         572        md  Markdown   \n","1   2    XML/000002.props       3013          44     props       XML   \n","2   3     Text/000003.txt       1076          21       txt      Text   \n","3   4  Markdown/000004.md       8105          84        md  Markdown   \n","4   5  Markdown/000005.md       2763          41        md  Markdown   \n","\n","                                                code  \\\n","0  # Contributing\\n\\n| Component            | Bui...   \n","1  ﻿<Project ToolsVersion=\"15.0\" xmlns=\"http://sc...   \n","2  The MIT License (MIT)\\n\\nCopyright (c) 2015 Mi...   \n","3  # Azure SDK for .NET\\n\\n[![Packages](https://i...   \n","4  <!-- BEGIN MICROSOFT SECURITY.MD V0.0.5 BLOCK ...   \n","\n","                                          clean_code  clean_line_count  \\\n","0   contributing\\n\\n component             build ...               186   \n","1  project toolsversion xmlns\\n  propertygroup\\n ...                44   \n","2  the mit license mit\\n\\ncopyright c 2015 micros...                21   \n","3   azure sdk for net\\n\\npackageshttpsimgshieldsi...                84   \n","4   begin microsoft securitymd v005 block \\n\\n se...                41   \n","\n","   clean_size  \n","0       10000  \n","1        1812  \n","2        1026  \n","3        7244  \n","4        2523  "]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["# Loading the dataset\n","import pandas as pd\n","df = pd.read_csv('dataset.csv')\n","df.head()"]},{"cell_type":"code","execution_count":null,"id":"9524b01f","metadata":{"id":"9524b01f"},"outputs":[],"source":["# Preprocessing dataset\n","df = df.dropna(subset=['clean_code', 'language']) # Drop rows with missing values in 'clean_code' or 'language' columns\n","\n","# Remove classes with only one sample in target column 'language'\n","class_counts = df['language'].value_counts()\n","df = df[df['language'].isin(class_counts[class_counts > 1].index)]"]},{"cell_type":"code","execution_count":null,"id":"578ff9a7","metadata":{"id":"578ff9a7"},"outputs":[],"source":["# Define Features (X) and Labels (y)\n","X = df['clean_code']\n","y = df['language']\n","\n","# Split the data into training and testing sets\n","from sklearn.model_selection import train_test_split\n","X_train, X_test, y_train, y_test = train_test_split(\n","    X, y, test_size=0.2, random_state=42, stratify=y\n",")"]},{"cell_type":"code","execution_count":null,"id":"e35549a9","metadata":{"id":"e35549a9"},"outputs":[],"source":["# TF-IDF Vectorization, Label Encoding, and Tensor Conversion\n","# TF-IDF Vectorization\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","vectorizer = TfidfVectorizer(max_features=1000)\n","X_train_features = vectorizer.fit_transform(X_train).toarray()\n","X_test_features = vectorizer.transform(X_test).toarray()\n","\n","# Encode labels to integers\n","from sklearn.preprocessing import LabelEncoder\n","encoder = LabelEncoder()\n","y_train_encoded = encoder.fit_transform(y_train)\n","y_test_encoded = encoder.transform(y_test)\n","NUM_CLASSES = len(encoder.classes_)\n","INPUT_DIM = X_train_features.shape[1]\n","\n","# Convert sparse SciPy matrices to dense NumPy, then to PyTorch tensors\n","# NOTE: PyTorch models generally expect float32\n","X_train_dense = X_train_features\n","X_test_dense = X_test_features\n","\n","# Convert to PyTorch Tensors\n","import torch\n","X_train_tensor = torch.tensor(X_train_dense, dtype=torch.float32)\n","y_train_tensor = torch.tensor(y_train_encoded, dtype=torch.long)\n","X_test_tensor = torch.tensor(X_test_dense, dtype=torch.float32)\n","y_test_tensor = torch.tensor(y_test_encoded, dtype=torch.long)\n","\n","# Create TensorDatasets and DataLoaders for batch training\n","from torch.utils.data import TensorDataset, DataLoader\n","train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n","train_loader = DataLoader(dataset=train_dataset, batch_size=64, shuffle=True)\n"]},{"cell_type":"code","execution_count":null,"id":"3242e079","metadata":{"id":"3242e079"},"outputs":[],"source":["# Model Definition: Multinomial Logistic Regression\n","import torch.nn as nn\n","class PyTorchLogisticRegression(nn.Module):\n","    \"\"\"\n","    Implements Logistic Regression as a single-layer Neural Network\n","    for multi-class classification.\n","    \"\"\"\n","    def __init__(self, input_dim, num_classes):\n","        super().__init__()\n","        self.linear = nn.Linear(input_dim, num_classes)\n","\n","    def forward(self, x):\n","        \"\"\"\n","        Returns:\n","            output_logits: Tensor of shape (batch_size, num_classes)\n","        \"\"\"\n","        output_logits = self.linear(x)\n","        return output_logits\n","\n","# Model Initialization: Multinomial Logistic Regression\n","model_pt = PyTorchLogisticRegression(INPUT_DIM, NUM_CLASSES)\n","\n","# Loss function () and Optimizer Definition\n","criterion = nn.CrossEntropyLoss() # CrossEntropyLoss includes Softmax\n","optimizer = torch.optim.Adam(model_pt.parameters(), lr=0.001)"]},{"cell_type":"code","execution_count":null,"id":"08df5bbd","metadata":{"id":"08df5bbd","outputId":"1d444b21-1664-490e-b4ee-994acf54c511"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch [1/10], Loss: 0.0423\n","Epoch [2/10], Loss: 1.0477\n","Epoch [2/10], Loss: 1.0477\n","Epoch [3/10], Loss: 0.3827\n","Epoch [3/10], Loss: 0.3827\n","Epoch [4/10], Loss: 0.1869\n","Epoch [4/10], Loss: 0.1869\n","Epoch [5/10], Loss: 0.6076\n","Epoch [5/10], Loss: 0.6076\n","Epoch [6/10], Loss: 0.0855\n","Epoch [6/10], Loss: 0.0855\n","Epoch [7/10], Loss: 0.0626\n","Epoch [7/10], Loss: 0.0626\n","Epoch [8/10], Loss: 0.1061\n","Epoch [8/10], Loss: 0.1061\n","Epoch [9/10], Loss: 0.5071\n","Epoch [9/10], Loss: 0.5071\n","Epoch [10/10], Loss: 0.6527\n","Epoch [10/10], Loss: 0.6527\n"]}],"source":["# Training Model: Multinomial Logistic Regression\n","num_epochs = 10\n","model_pt.train()\n","\n","# Training Loop\n","for epoch in range(num_epochs):\n","    for i, (features, labels) in enumerate(train_loader):\n","        # Forward pass\n","        outputs = model_pt(features)\n","        loss = criterion(outputs, labels)\n","\n","        # Backward and optimize\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')"]},{"cell_type":"code","execution_count":null,"id":"78f7cf10","metadata":{"id":"78f7cf10","outputId":"0fd823ec-f32d-410c-8f24-801575094da1"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","--- PyTorch Logistic Regression Results ---\n","Accuracy: 0.9416\n","F1-Score: 0.9372\n","Confusion Matrix (Labels are 0, 1, 2...):\n"," [[ 9  0  0 ...  0  0  0]\n"," [ 0  0  0 ...  0  0  0]\n"," [ 0  0  6 ...  0  0  0]\n"," ...\n"," [ 0  0  0 ...  0  0  0]\n"," [ 0  0  0 ...  0 64  0]\n"," [ 0  0  0 ...  0  0 39]]\n"]}],"source":["# Model Evaluation: Multinomial Logistic Regression\n","model_pt.eval()\n","with torch.no_grad():\n","    outputs = model_pt(X_test_tensor) # Get predictions on the test set\n","    _, y_pred_pt = torch.max(outputs.data, 1) # The class prediction is the index with the highest score (logit)\n","    y_pred_np = y_pred_pt.numpy() # Convert predictions back to numpy for scikit-learn metrics\n","\n","    # Calculate Metrics and display results\n","    from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n","    acc_pt = accuracy_score(y_test_encoded, y_pred_np)\n","    f1_pt = f1_score(y_test_encoded, y_pred_np, average='weighted')\n","    print(\"\\n--- PyTorch Logistic Regression Results ---\")\n","    print(f\"Accuracy: {acc_pt:.4f}\")\n","    print(f\"F1-Score: {f1_pt:.4f}\")\n","    print(\"Confusion Matrix (Labels are 0, 1, 2...):\\n\", confusion_matrix(y_test_encoded, y_pred_np))"]},{"cell_type":"code","execution_count":null,"id":"e961a2bf","metadata":{"id":"e961a2bf"},"outputs":[],"source":["# Prediction Function for Multinomial Logistic Regression\n","def predict_multinomial(model, vectorizer, new_texts):\n","    \"\"\"\n","    Predict labels for new texts using a multinomial model.\n","    Args:\n","        model: Trained multinomial classification model (e.g., with Softmax output)\n","        vectorizer: Fitted TfidfVectorizer\n","        new_texts: List of text strings to classify\n","    Returns:\n","        predictions: Predicted class indices\n","        probabilities: Probability scores for each class\n","    \"\"\"\n","\n","    # Transform new texts using the same vectorizer\n","    X_new = vectorizer.transform(new_texts).toarray()\n","    X_new_tensor = torch.tensor(X_new, dtype=torch.float32)\n","\n","    model.eval() # Set model to evaluation mode\n","\n","    # Make predictions\n","    with torch.no_grad():\n","        outputs = model(X_new_tensor)\n","        probabilities = outputs # Model outputs scores/logits or probabilities\n","        predictions = torch.argmax(probabilities, dim=1) # Get the class with the highest probability\n","\n","    return predictions, probabilities\n","\n","# Note: To use this function, you would need a multinomial model trained with\n","# categorical cross-entropy loss and an appropriate output layer.\n","# The rest of your data loading and vectorization steps would remain similar."]},{"cell_type":"code","execution_count":null,"id":"242d2b35","metadata":{"id":"242d2b35","outputId":"ad3555ee-be3a-4e53-ddbd-d665e808d1e6"},"outputs":[{"name":"stdout","output_type":"stream","text":["['Predicted Programming Language: Markdown']\n","tensor([[-13.0162, -12.4387,  -9.7510,  -7.1578,  -6.6710,  -7.4301, -15.5958,\n","         -10.5104, -12.2967,  -9.4380, -13.2916, -15.0997, -17.1523,  -7.5103,\n","         -13.0159,  -6.3105, -17.0187, -13.9976,  -5.7426, -14.7558, -14.2164,\n","         -16.7827, -12.7178, -16.3238,  -4.7670, -12.0553, -18.7029, -16.6741,\n","          -6.9039, -16.0770, -13.7410, -10.7152,  -6.8575,  -5.7670,  -8.3074,\n","         -14.4544,  -8.4775, -16.0846, -12.7949,  -7.1392,  -9.9650, -17.4416,\n","          -5.6903,   1.2377,  -8.3514,  -7.3445, -14.5968,  -3.3790,  -9.0177,\n","         -10.0762, -16.9868, -16.3285,  -5.2939,  -9.7186,  -8.8361,  -6.4836,\n","          -7.4861, -15.8865,  -8.8667, -17.3866, -14.9850, -13.5604,  -2.9508,\n","         -13.0561, -14.2647,  -7.9215, -14.9352,  -3.2596, -14.4701,  -4.8385,\n","         -15.5534,  -4.6419]])\n"]}],"source":["# Example usage of Prediction Function\n","row_series = df[df['language'] == 'Markdown'].iloc[0] # Extracting a sample row\n","new_texts = [row_series['clean_code']]\n","preds_tensor, probs = predict_multinomial(model_pt, vectorizer, new_texts)\n","preds_np = preds_tensor.numpy() # preds_tensor is a torch tensor of class indices\n","predicted_labels = encoder.inverse_transform(preds_np) # Convert indices back to original labels\n","print(\"Predicted Programming Language: \" + predicted_labels)\n","print(probs)"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.6"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}